{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AnalitemTCT**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPARACIÓN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AMBIENTE Y BASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instalar paquetes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import math\n",
    "import base64\n",
    "import datetime\n",
    "import base64\n",
    "from scipy.stats import pearsonr\n",
    "import sys\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import pingouin as pg\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subir el archivo de la prueba\n",
    "df_prueba = pd.read_excel(\"C:/Users/Desktop/AXS000/AXS_000.xlsx\")\n",
    "pd.options.display.max_columns = 200\n",
    "df_prueba.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subir el archivo de respuestas forms\n",
    "df = pd.read_excel(\"C:/Users/Desktop/AXS000/Piloto2024.xlsx\")\n",
    "pd.options.display.max_columns = 200\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELIMINAR PERSONAS QUE NO ACEPTARON POLÍTICA DE DATOS Y COLUMNAS NO REQUERIDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la lista de categorías únicas en la columna \"POBEDAD\"\n",
    "categorias = df['¿Apruebas el uso de tus datos personales?'].unique()\n",
    "\n",
    "print (categorias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['¿Apruebas el uso de tus datos personales?'] != 'No apruebo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminar las filas de personas repetidas, dejando la primera respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina filas duplicadas basadas en la columna 'Dirección de correo electrónico'\n",
    "df_sin_duplicados = df.drop_duplicates(subset='Dirección de correo electrónico', keep='first')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPARACIÓN DE BASES PARA CREAR DF CALIFICADO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DETECTAR LOS DF PRESENTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%who DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELIMINAR LAS COLUMNAS EN DF_OPCIONES PARA CREAR EL DICCIONARIO DE RESPUESTAS CORRECTAS E INCORRECTAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_prueba.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ELIMINAR COLUMNAS PARA SACAR DICCIONARIO\n",
    "# Lista de columnas a eliminar\n",
    "columnas_a_eliminar = ['#', 'CÓDIGO DEL ÍTEM', 'ENUNCIADO',]\n",
    "\n",
    "# Eliminar las columnas especificadas\n",
    "df_opciones = df_prueba.drop(columnas_a_eliminar, axis=1)\n",
    "\n",
    "print(df_opciones.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREAR EL DICCIONARIO\n",
    "# Obtener las columnas de respuestas en df_opciones\n",
    "columnas_respuestas = df_opciones.columns\n",
    "\n",
    "# Crear el diccionario de mapeo de manera automática\n",
    "mapeo_respuestas = {}\n",
    "for columna in columnas_respuestas:\n",
    "    mapeo_respuestas.update({texto_pregunta: columna for texto_pregunta in df_opciones[columna]})\n",
    "\n",
    "print(mapeo_respuestas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREAR COPIA PARA TENER UN DF CALIFICADO Y REEMPLAZAR CON EL DICCIONARIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Crear una copia de DF1 para mantener el original intacto\n",
    "df_calificado = df.copy() \n",
    "\n",
    "for fila in df_calificado.index:\n",
    "    for columna in df_calificado.columns:\n",
    "        # Verificar si el valor actual está en el diccionario de mapeo_respuestas\n",
    "        valor_actual = df_calificado.at[fila, columna]\n",
    "        if valor_actual in mapeo_respuestas:\n",
    "            # Utilizar el diccionario para reemplazar el valor actual\n",
    "            df_calificado.at[fila, columna] = mapeo_respuestas[valor_actual]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERIFICAR LOS VALORES ÚNICOS PARA LAS COLUMNAS DE LOS ÍTEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de nombres de columnas de interés\n",
    "columnas_de_interes = ['AXS001_NC_1', 'AXS001_ND_2', 'AXS001_PC_3', 'AXS001_PD_4', 'AXS001_ND_6', 'AXS001_PC_7', 'AXS001_PD_8', 'AXS001_NC_9', 'AXS001_ND_10', 'AXS001_PD_12', 'AXS001_NC_13', 'AXS001_PC_15', 'AXS001_PD_16', 'AXS001_NC_17', 'AXS001_ND_18', 'AXS001_PC_19', 'AXS001_PD_20', 'AXS001_NC_21', 'AXS001_ND_22', 'AXS001_PC_23', 'AXS001_PD_24', 'AXS001_NC_25', 'AXS001_ND_26', 'AXS001_PC_27', 'AXS001_PD_28', 'AXS001_ND_29', 'AXS001_NC_30', 'AXS001_ND_31', 'AXS001_PD_33', 'AXS001_PC_34', 'AXS001_PD_35', 'AXS001_ND_37', 'AXS001_PC_38', 'AXS001_PD_39', 'AXS001_NC_40', 'AXS001_NC_44', 'AXS001_ND_45', 'AXS001_PC_46', 'AXS001_PD_47', 'AXS001_NC_48', 'AXS001_ND_49', 'AXS001_PC_50', 'AXS001_PC_52', 'AXS001_PC_53', 'AXS001_ND_54', 'AXS001_PC_55', 'AXS001_PD_56', 'AXS001_NC_57', 'AXS001_ND_58', 'AXS001_PC_59', 'AXS001_PD_60', 'AXS001_NC_61', 'AXS001_PD_62', 'AXS001_PC_63', 'AXS001_PD_64', 'AXS001_NC_65', 'AXS001_ND_66', 'AXS001_PC_67', 'AXS001_PD_68', 'AXS001_NC_69', 'AXS001_ND_70']\n",
    "\n",
    "# Inicializa un conjunto vacío para almacenar los valores únicos\n",
    "valores_unicos = set()\n",
    "\n",
    "# Itera a través de las columnas de interés y agrega los valores únicos al conjunto\n",
    "for columna in columnas_de_interes:\n",
    "    valores_unicos.update(df_calificado[columna].unique())\n",
    "\n",
    "# Los valores únicos ahora se encuentran en el conjunto 'valores_unicos'\n",
    "print(valores_unicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reemplazar los que siguen apareciendo con el texto\n",
    "\n",
    "#df_calificado = df_calificado.replace('', 'RESPUESTA CORRECTA', regex=True)\n",
    "\n",
    "#VERIFICAR CORRIENDO LA FILA ANTERIOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SACAR FRECUENCIA DE RESPUESTAS POR COLUMNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELIMINAR COLUMNAS QUE NO SE REQUIEREN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_calificado.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ELIMINAR COLUMNAS PARA SACAR DICCIONARIO\n",
    "# Lista de columnas a eliminar\n",
    "columnas_a_eliminar = ['Dirección de correo electrónico', 'Puntuación', '¿En qué país vives actualmente?', '¿Qué edad tienes?', '¿Cuál es tu nivel de escolaridad?', '¿Cuál es tu sexo biológico?', '¿Cuál de las siguientes opciones representa mejor tu identidad de género?', '¿En cuál de las siguientes áreas trabajas actualmente?']\n",
    "\n",
    "# Eliminar las columnas especificadas\n",
    "df_calificado = df_calificado.drop(columnas_a_eliminar, axis=1)\n",
    "\n",
    "print(df_calificado.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the categories in each column\n",
    "dffrecuency = pd.concat([\n",
    "    pd.Series(df_calificado[col].value_counts(), name=col).sort_index()\n",
    "    for col in df_calificado.columns\n",
    "], axis=1)\n",
    "\n",
    "# Transpose the information\n",
    "dffrecuency = dffrecuency.T\n",
    "\n",
    "# Add the category column\n",
    "dffrecuency.index.name = 'category'\n",
    "dffrecuency = dffrecuency.reset_index()\n",
    "\n",
    "#Change the type of the name columns ti string\n",
    "dffrecuency.columns = dffrecuency.columns.astype(str)\n",
    "\n",
    "dffrecuency.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREAR DF_BINARIA Y DESCRIPTIVOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binaria = df_calificado.replace('RESPUESTA CORRECTA','1', regex=True)\n",
    "df_binaria = df_binaria.replace('RESPUESTA INCORRECTA.1','0', regex=True)\n",
    "df_binaria = df_binaria.replace('RESPUESTA INCORRECTA','0', regex=True)\n",
    "\n",
    "\n",
    "df_binaria = df_binaria.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sacar descriptivos para analitem\n",
    "dfdescribe=df_binaria.describe()\n",
    "\n",
    "# Transpose the information\n",
    "dfdescribe = dfdescribe.T\n",
    "\n",
    "# Add the category column\n",
    "dfdescribe.index.name = 'category'\n",
    "dfdescribe = dfdescribe.reset_index()\n",
    "\n",
    "dfdescribe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNIR LA FRENCUENCIA DE RESPUESTAS CON LOS DESCRIPTIVOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir la función BuscarX\n",
    "def xlookup(lookup_value, lookup_array, return_array, if_not_found:str = ''):\n",
    "    match_value = return_array.loc[lookup_array == lookup_value]\n",
    "    if match_value.empty:\n",
    "        return f'\"{lookup_value}\" not found!' if if_not_found == '' else if_not_found\n",
    "\n",
    "    else:\n",
    "        return match_value.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dffrecuency.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traer frecuencia de ítems\n",
    "dfdescribe['CORRECTA'] = dfdescribe['category'].apply(xlookup, args = (dffrecuency['category'], dffrecuency['RESPUESTA CORRECTA']))\n",
    "dfdescribe['INCORRECTA1'] = dfdescribe['category'].apply(xlookup, args = (dffrecuency['category'], dffrecuency['RESPUESTA INCORRECTA']))\n",
    "dfdescribe['INCORRECTA2'] = dfdescribe['category'].apply(xlookup, args = (dffrecuency['category'], dffrecuency['RESPUESTA INCORRECTA.1']))\n",
    "dfdescribe.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SACAR DIFICULTAD Y DISCRIMINACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculo de puntuación por evaluado\n",
    "punt_evaluado = df_binaria.sum(axis=1,numeric_only=True)\n",
    "\n",
    "#Calculo de puntuacion por item\n",
    "punt_item = df_binaria.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dificultad\n",
    "Diff = punt_item/len(df_binaria)\n",
    "items = Diff.index.tolist()\n",
    "dificultad = Diff.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dificultad corregida\n",
    "q=1-Diff\n",
    "DiffCorr=Diff-(q/2)\n",
    "items = DiffCorr.index.tolist()\n",
    "dificultad_corr = DiffCorr.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculo correlacion biserial puntual por item\n",
    "CorrBP=df_binaria[['AXS001_NC_1', 'AXS001_ND_2', 'AXS001_PC_3', 'AXS001_PD_4', 'AXS001_ND_6', 'AXS001_PC_7', 'AXS001_PD_8', 'AXS001_NC_9', 'AXS001_ND_10', 'AXS001_PD_12', 'AXS001_NC_13', 'AXS001_PC_15', 'AXS001_PD_16', 'AXS001_NC_17', 'AXS001_ND_18', 'AXS001_PC_19', 'AXS001_PD_20', 'AXS001_NC_21', 'AXS001_ND_22', 'AXS001_PC_23', 'AXS001_PD_24', 'AXS001_NC_25', 'AXS001_ND_26', 'AXS001_PC_27', 'AXS001_PD_28', 'AXS001_ND_29', 'AXS001_NC_30', 'AXS001_ND_31', 'AXS001_PD_33', 'AXS001_PC_34', 'AXS001_PD_35', 'AXS001_ND_37', 'AXS001_PC_38', 'AXS001_PD_39', 'AXS001_NC_40', 'AXS001_NC_44', 'AXS001_ND_45', 'AXS001_PC_46', 'AXS001_PD_47', 'AXS001_NC_48', 'AXS001_ND_49', 'AXS001_PC_50', 'AXS001_PC_52', 'AXS001_PC_53', 'AXS001_ND_54', 'AXS001_PC_55', 'AXS001_PD_56', 'AXS001_NC_57', 'AXS001_ND_58', 'AXS001_PC_59', 'AXS001_PD_60', 'AXS001_NC_61', 'AXS001_PD_62', 'AXS001_PC_63', 'AXS001_PD_64', 'AXS001_NC_65', 'AXS001_ND_66', 'AXS001_PC_67', 'AXS001_PD_68', 'AXS001_NC_69', 'AXS001_ND_70']].corrwith(punt_evaluado)\n",
    "method=stats.pointbiserialr\n",
    "\n",
    "items = CorrBP.index.tolist()\n",
    "corr_biserial = CorrBP.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir todas las listas en una sola\n",
    "data = {\n",
    "    'Ítem': items,\n",
    "    'Dificultad': dificultad,\n",
    "    'Dificultad Corregida': dificultad_corr,\n",
    "    'Correlación Biserial': corr_biserial\n",
    "}\n",
    "# Convertir la lista en df\n",
    "DifDisc = pd.DataFrame(data)\n",
    "\n",
    "print(DifDisc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traer frecuencia de ítems\n",
    "dfdescribe['DIFICULTAD'] = dfdescribe['category'].apply(xlookup, args = (DifDisc['Ítem'], DifDisc['Dificultad']))\n",
    "dfdescribe['DIFICULTAD CORR'] = dfdescribe['category'].apply(xlookup, args = (DifDisc['Ítem'], DifDisc['Dificultad Corregida']))\n",
    "dfdescribe['CORR BIS'] = dfdescribe['category'].apply(xlookup, args = (DifDisc['Ítem'], DifDisc['Correlación Biserial']))\n",
    "dfdescribe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SACR FIABILIDAD Y VALIDEZ CON BINARIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%who DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fiabilidad y Validez**\n",
    "### CronbachAlpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that 'df_binaria' is a pandas DataFrame containing your item scores\n",
    "alpha = pg.cronbach_alpha(data=df_binaria)\n",
    "print(\"Cronbach's alpha:\", alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Create a PCA object with the desired number of components\n",
    "pca = PCA(n_components=4)\n",
    "\n",
    "# Fit the PCA model to the data\n",
    "pca.fit(df_binaria)\n",
    "\n",
    "# Get the explained variance ratio of all the principal components\n",
    "variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Export the explained percentage variability of all the principal components\n",
    "for i, ratio in enumerate(variance_ratio):\n",
    "    print(f\"Explained variance ratio of principal component {i+1}: {ratio*100:.2f}%\")\n",
    "\n",
    "cumulative_ratio = np.cumsum(variance_ratio)\n",
    "\n",
    "# Export the cumulative variance ratio\n",
    "for i, ratio in enumerate(cumulative_ratio):\n",
    "    print(f\"Cumulative explained variance ratio up to principal component {i+1}: {ratio*100:.2f}%\")\n",
    "\n",
    "# Create a DataFrame with the component loadings\n",
    "component_loadings = pd.DataFrame(pca.components_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%who DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRAER RESULTADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "# Create a new Excel workbook\n",
    "workbook = Workbook()\n",
    "\n",
    "# Create separate sheets in the workbook for each DataFrame\n",
    "workbook.create_sheet(title='Calificado', index=0)\n",
    "workbook.create_sheet(title='Binaria', index=1)\n",
    "workbook.create_sheet(title='Descriptivos', index=2)\n",
    "workbook.create_sheet(title='PCA_Loadings', index=3)\n",
    "\n",
    "# Get sheet objects\n",
    "sheet_calificado = workbook['Calificado']\n",
    "sheet_binaria = workbook['Binaria']\n",
    "sheet_descriptivos = workbook['Descriptivos']\n",
    "sheet_pca_loadings = workbook['PCA_Loadings']\n",
    "\n",
    "# Write DataFrames to respective sheets\n",
    "for r_idx, row in enumerate(dataframe_to_rows(df_calificado, index=False), 1):\n",
    "    sheet_calificado.append(row)\n",
    "\n",
    "for r_idx, row in enumerate(dataframe_to_rows(df_binaria, index=False), 1):\n",
    "    sheet_binaria.append(row)\n",
    "\n",
    "for r_idx, row in enumerate(dataframe_to_rows(dfdescribe, index=False), 1):\n",
    "    sheet_descriptivos.append(row)\n",
    "\n",
    "for r_idx, row in enumerate(dataframe_to_rows(component_loadings, index=False), 1):\n",
    "    sheet_pca_loadings.append(row)\n",
    "\n",
    "# Save the workbook\n",
    "workbook.save('C:/Users/Desktop/AXS000/RESULTADOS_LM.xlsx')\n",
    "\n",
    "workbook.close\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una copia del DataFrame\n",
    "df_binarialimpia = df_calificado.copy()\n",
    "\n",
    "# Lista de columnas a eliminar\n",
    "columnas_a_eliminar = [\n",
    "    'AXS001_ND_2','AXS001_ND_6','AXS001_PD_12','AXS001_NC_13','AXS001_PD_24','AXS001_PD_28','AXS001_PD_33','AXS001_PD_35','AXS001_NC_57','AXS001_NC_61','AXS001_PC_63','AXS001_NC_1','AXS001_PC_3','AXS001_PD_4','AXS001_PC_7','AXS001_PD_8','AXS001_NC_9','AXS001_ND_10','AXS001_PC_15','AXS001_PD_16','AXS001_ND_18','AXS001_PD_20','AXS001_ND_22','AXS001_NC_25','AXS001_ND_29','AXS001_NC_30','AXS001_PC_34','AXS001_PC_38','AXS001_PD_39','AXS001_NC_40','AXS001_ND_45','AXS001_PC_46','AXS001_PD_47','AXS001_ND_49','AXS001_PC_50','AXS001_ND_58','AXS001_PD_60'\n",
    "]\n",
    "\n",
    "# Mostrar las columnas del DataFrame original\n",
    "print(\"Columnas del DataFrame original:\")\n",
    "print(df_binarialimpia.columns.tolist())\n",
    "\n",
    "# Verificar que las columnas a eliminar existen en el DataFrame\n",
    "columnas_existentes = [col for col in columnas_a_eliminar if col in df_binarialimpia.columns]\n",
    "columnas_no_existentes = [col for col in columnas_a_eliminar if col not in df_binarialimpia.columns]\n",
    "\n",
    "print(\"\\nColumnas a eliminar que existen en el DataFrame:\")\n",
    "print(columnas_existentes)\n",
    "\n",
    "print(\"\\nColumnas a eliminar que NO existen en el DataFrame:\")\n",
    "print(columnas_no_existentes)\n",
    "\n",
    "# Eliminar las columnas que existen en el DataFrame\n",
    "df_binarialimpia = df_binarialimpia.drop(columnas_existentes, axis=1)\n",
    "\n",
    "# Mostrar las columnas del DataFrame después de eliminar\n",
    "print(\"\\nColumnas del DataFrame después de eliminar las especificadas:\")\n",
    "print(df_binarialimpia.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Excel workbook\n",
    "workbook = Workbook()\n",
    "# Create separate sheets in the workbook for each DataFrame\n",
    "workbook.create_sheet(title='Binaria', index=1)\n",
    "\n",
    "# Get sheet objects\n",
    "sheet_binaria = workbook['Binaria']\n",
    "\n",
    "# Write DataFrames to respective sheets\n",
    "for r_idx, row in enumerate(dataframe_to_rows(df_binarialimpia, index=False), 1):\n",
    "    sheet_binaria.append(row)\n",
    "\n",
    "# Save the workbook\n",
    "workbook.save('C:/Users/Desktop/AXS000/Binarialimpia.xlsx')\n",
    "workbook.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminación de ítems posterior a Analitem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binariaanalitem = df_binaria.copy() \n",
    "print(df_binariaanalitem.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas a eliminar\n",
    "columnas_a_eliminar = ['AXS001_ND_2','AXS001_ND_10','AXS001_PC_63','AXS001_PD_4','AXS001_NC_57','AXS001_NC_61','AXS001_NC_65','AXS001_ND_70','AXS001_ND_66','AXS001_PD_28','AXS001_PD_33','AXS001_PD_35','AXS001_PD_24','AXS001_ND_6','AXS001_NC_13','AXS001_PC_7','AXS001_PD_12','AXS001_ND_49','AXS001_PC_67','AXS001_PC_27','AXS001_ND_22','AXS001_PC_50','AXS001_NC_1','AXS001_NC_25','AXS001_NC_30','AXS001_NC_40','AXS001_NC_9','AXS001_ND_18','AXS001_ND_22','AXS001_ND_29','AXS001_ND_45','AXS001_ND_49','AXS001_ND_58','AXS001_PC_34','AXS001_PC_38','AXS001_PC_46','AXS001_PD_16','AXS001_PD_20','AXS001_PD_39','AXS001_PD_47','AXS001_PD_60','AXS001_PD_8']\n",
    "\n",
    "# Eliminar las columnas especificadas\n",
    "df_binariaanalitem = df_binariaanalitem.drop(columnas_a_eliminar, axis=1)\n",
    "\n",
    "print(df_binariaanalitem.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cronbach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that 'df_binaria' is a pandas DataFrame containing your item scores\n",
    "alpha = pg.cronbach_alpha(data=df_binaria)\n",
    "print(\"Cronbach's alpha:\", alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Create a PCA object with the desired number of components\n",
    "pca = PCA(n_components=4)\n",
    "\n",
    "# Fit the PCA model to the data\n",
    "pca.fit(df_binariaanalitem)\n",
    "\n",
    "# Get the explained variance ratio of all the principal components\n",
    "variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Export the explained percentage variability of all the principal components\n",
    "for i, ratio in enumerate(variance_ratio):\n",
    "    print(f\"Explained variance ratio of principal component {i+1}: {ratio*100:.2f}%\")\n",
    "\n",
    "cumulative_ratio = np.cumsum(variance_ratio)\n",
    "\n",
    "# Export the cumulative variance ratio\n",
    "for i, ratio in enumerate(cumulative_ratio):\n",
    "    print(f\"Cumulative explained variance ratio up to principal component {i+1}: {ratio*100:.2f}%\")\n",
    "\n",
    "# Create a DataFrame with the component loadings\n",
    "component_loadings = pd.DataFrame(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Excel workbook\n",
    "workbook = Workbook()\n",
    "\n",
    "# Create separate sheets in the workbook for each DataFrame\n",
    "workbook.create_sheet(title='BinariaPostanalitem', index=0)\n",
    "\n",
    "# Get sheet objects\n",
    "sheet_binaria = workbook['BinariaPostanalitem']\n",
    "\n",
    "# Write DataFrames to respective sheets\n",
    "for r_idx, row in enumerate(dataframe_to_rows(df_binariaanalitem, index=False), 1):\n",
    "    sheet_binaria.append(row)\n",
    "\n",
    "# Save the workbook\n",
    "workbook.save('C:/Users/Desktop/AXS000/VF_AXS_001.xlsx')\n",
    "\n",
    "workbook.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CERRADA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
